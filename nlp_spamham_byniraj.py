# -*- coding: utf-8 -*-
"""NLP_SpamHam_ByNiraj.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SCaCY_wIp-ZrefTxsmO-Sy5sn-MCJ28P

# Importing libraries
"""

import pandas as pd
import numpy as np
import os
import re
from sklearn.preprocessing import StandardScaler
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import  plot_roc_curve, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve, auc
from gensim.models import Word2Vec

"""# Reading the text file"""

file_path = r'C:\Users\rokay\Downloads\SpamDetection\SpamDetection'

def read_email_file(file_path):
    with open(file_path, 'r', encoding='latin-1') as file:
        content = file.read()
    return content

ham_folder = r"C:\Users\rokay\Downloads\SpamDetection\SpamDetection\ham"
spam_folder = r"C:\Users\rokay\Downloads\SpamDetection\SpamDetection\spam"

ham_files = [os.path.join(ham_folder, file) for file in os.listdir(ham_folder)]
spam_files = [os.path.join(spam_folder, file) for file in os.listdir(spam_folder)]

ham_data = [read_email_file(file) for file in ham_files]
spam_data = [read_email_file(file) for file in spam_files]

"""# Making dataframe out of the text file and assigning labels for them"""

# Assuming 'ham' is 0 and 'spam' is 1
df = pd.DataFrame({'text': ham_data + spam_data, 'label': [0] * len(ham_data) + [1] * len(spam_data)})
df

"""# Using Regx"""

# How many records have a date that is expressed without using alphabets?
pattern_date = r'\b\d{1,2}[/-]\d{1,2}[/-]\d{2,4}\b'
date_without_alphabets = df['text'].str.count(pattern_date).sum()

# Count records that make a word that starts with an alphabet and is not a URL.
pattern_non_url = r'\b(?![\w\d]+:\/\/)[a-zA-Z]\w*\b'
count_non_url = df['text'].str.count(pattern_non_url).sum()

#  Count tweets that contain one of these emojis :), :D, ;), :P.
pattern_emojis = r'[:;][-]?[)DP]'
count_emojis = df['text'].str.count(pattern_emojis).sum()

#  Count records that contain a decimal number.
pattern_decimal = r'\b\d+\.\d+\b'
count_decimal = df['text'].str.count(pattern_decimal).sum()

# Count the total number of IP addresses across all the records.
pattern_ip = r'\b\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}\b'
count_ip = df['text'].str.count(pattern_ip).sum()

# Count the total number of hashtags across all these tweets.
pattern_hashtags = r'#[\w\d_]+'
count_hashtags = df['text'].str.count(pattern_hashtags).sum()

# Count the total number of URLs across all tweets.
pattern_url = r'https?://\S+'
count_url = df['text'].str.count(pattern_url).sum()

# Email length and capital letter usage
email_length= df['text'].apply(len)
capital_count= df['text'].str.count(r'[A-Z]')

print(date_without_alphabets,count_non_url, count_emojis, count_decimal, count_ip,count_hashtags, count_url,email_length,capital_count)

df.isna().sum() # checking if there are any null values in dataframe

"""# Deriving different feature columns from the Regx"""

df['date_count'] = df['text'].str.count(pattern_date)
df['non_url_count'] = df['text'].str.count(pattern_non_url)
df['emoji_count'] = df['text'].str.count(pattern_emojis)
df['decimal_count'] = df['text'].str.count(pattern_decimal)
df['ip_count'] = df['text'].str.count(pattern_ip)
df['hashtag_count'] = df['text'].str.count(pattern_hashtags)
df['url_count'] = df['text'].str.count(pattern_url)

"""Email length and capital letter usage"""

df['email_length'] = df['text'].apply(len)
df['capital_count'] = df['text'].str.count(r'[A-Z]')

"""Ratios:
Ratio of capital letters to email length.
Ratio of non-URL words to total words.
"""

df['capital_to_length_ratio'] = df['capital_count'] / df['email_length']
df['non_url_to_total_ratio'] = df['non_url_count'] / df['email_length']

"""Presence of Common Spam Phrases:"""

common_spam_phrases = ['win money', 'urgent action required', 'click below', 'congratulations', 'you have won']
for phrase in common_spam_phrases:
    df[f'has_phrase_{phrase.replace(" ", "_")}'] = df['text'].str.contains(phrase, case=False).astype(int)

"""Email Structure"""

df['html_count'] = df['text'].str.count(r'<.*?>')

"""Special char count"""

df['special_char_count'] = df['text'].str.count(r'[!@#$%^&*()]')

"""Word Density"""

# calculating the number of spaces in each email
df['space_count'] = df['text'].str.count(' ')

# Then, computing word density
df['word_density'] = df['non_url_count'] / (df['email_length'] - df['space_count'])

"""Word Count:"""

df['word_count'] = df['text'].apply(lambda x: len(str(x).split()))

"""Average Word Length: This can sometimes give clues about the nature of the content.


"""

df['avg_word_length'] = df['text'].apply(lambda x: sum(len(word) for word in str(x).split()) / len(str(x).split()))

"""Presence of Suspected Spam Keywords"""

spam_keywords = ['free', 'offer', 'win', 'discount', 'prize']
for keyword in spam_keywords:
    df[f'has_keyword_{keyword}'] = df['text'].str.contains(keyword, case=False).astype(int)

"""Presence of Special Patterns"""

df['exclamation_sequence'] = df['text'].str.count(r'!!+')
df['question_sequence'] = df['text'].str.count(r'\?\?+')

"""Check for Personalization"""

df['personalized_greeting'] = df['text'].str.contains(r'Dear [A-Z][a-z]+', regex=True).astype(int)

"""Check for Unsubscribe Links"""

df['has_unsubscribe'] = df['text'].str.contains(r'unsubscribe', case=False).astype(int)

# Function to count the occurrence of '1' in a cell
def count_ones(cell):
    # Convert the cell to a string and count '1's
    return str(cell).count('1')

# Apply the count_ones function to all numerical columns
word_count_with_1 = df.select_dtypes(include=['number']).applymap(count_ones)

# Sum the word counts for each column to get the total count
total_word_count_with_1 = word_count_with_1.sum()

# Display the word count for each column and the total count
print(f"Total Word Count with '1': {total_word_count_with_1}")

df.isna().sum()

df.dropna(axis = 0,inplace = True)

"""# Tokenization"""

# Tokenization function
def tokenize(text):
    # Using regex to split the text into tokens
    tokens = re.findall(r'\w+', text)
    return tokens

# Apply tokenization on the 'label' column
df['text'] = df['text'].apply(tokenize)

df.head()

"""checking for the nan values"""

df.isna().sum()

"""Understanding the data distribution"""

df.describe()

"""Summary:

Label: About 22% of emails are spam, indicating a slight imbalance.

Date Count: Most emails lack dates, but a few have up to 91 instances, potentially hinting at spam.

Emoji Count: Emojis are rare, though some emails have as many as 40.

Decimal Count: Emails average around 21 decimals, possibly from version numbers or IP fragments.

IP Count: Emails typically have around 5 IP addresses; some have up to 32.

Hashtag and URL Counts: High counts might hint at promotional content or spam.

Email Length & Capital Count: There's significant variance in length and capital usage, with some emails being notably long or having many capital letters.

Outliers: Some columns have values far beyond the 75th percentile, indicating special cases or anomalies.

# Visulaizing the data to identify the occurance of the data
"""

# Filter numerical columns based on data types
numerical_columns = df.select_dtypes(include=['int64', 'float64'])

# Create histograms for numerical columns by 'label' column
for column in numerical_columns.columns:
    plt.figure(figsize=(10, 6))

    # Plot histogram for 'Ham' emails (label=0)
    plt.hist(df[df['label'] == 0][column], bins=50, alpha=0.5, label='Ham')

    # Plot histogram for 'Spam' emails (label=1)
    plt.hist(df[df['label'] == 1][column], bins=50, alpha=0.5, label='Spam')

    plt.title(f'Histogram of {column} by Email Type')
    plt.xlabel(column)
    plt.ylabel('Frequency')
    plt.legend(loc='upper right')

    plt.show()

"""Summary:

Features like URL count, capital count, and hashtag count seem to have broader distributions for spam emails, suggesting variability in spam content.

Most emails, whether spam or ham, tend to have fewer dates, decimals, emojis, and IP addresses. However, there might still be subtle differences between the two categories.

Email length seems to be a potential distinguishing feature, with ham emails showing a more consistent length, while spam emails display more variability.

# Visualizing data to see the correlation between the features in heatmap
"""

correlation_matrix = df.corr()
plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.show()

"""Feature Importance: As seen from the chart, "Decimal Count," "IP Count," and "Hashtag Count" have relatively lower importance scores compared to other features. This suggests that they might not be as influential in the model's decision-making process.

Model Complexity: Removing features can simplify the model, making it faster and potentially reducing the risk of overfitting.

Interpretability: Fewer features can make the model easier to understand and explain.

Domain Knowledge: Sometimes, even if a feature has low importance, it might still be kept based on domain-specific reasons.

# Text Vectorization and Generating a Bag-of-Words (BoW) Representation of Text Data using CountVectorizer - Unigram
"""

data = df.sample(frac=0.40, random_state=42)

sample_df = data.drop(columns=['label'])
sample_df

# Joining the tokens into a single string as CountVectorizer expects string input
sample_df['text_joined'] = sample_df['text'].apply(' '.join)

# Initializing the CountVectorizer
vectorizer = CountVectorizer()

# Transforming the 'text_joined' column into a BoW representation
bow_matrix = vectorizer.fit_transform(sample_df['text_joined'])

# The BoW matrix can be converted back to a DataFrame for a clearer visualization
bow_df = pd.DataFrame(bow_matrix.toarray(), columns=vectorizer.get_feature_names_out())

bow_df

bow_df.drop(columns = ['label'])

bow_df.reset_index(drop=True, inplace=True)
sample_df.reset_index(drop=True, inplace=True)

"""# Creating a combined dataframe with original data and bow_df after performing bag of words"""

combined_df = pd.concat([sample_df, bow_df], axis=1)
combined_df.head()

combined_df = combined_df.drop(columns=['text','text_joined','label'])  # dropping text column
combined_df

non_numeric_columns = combined_df.select_dtypes(include=['object']).columns.tolist() # checking if there are other object column
non_numeric_columns

"""# Scaling the data to reduce variance"""

X = combined_df

# Initializing the scaler
scaler = StandardScaler()

# Fitting the scaler and transforming the features
X_scaled = scaler.fit_transform(X)

# Converting back to DataFrame for clearer visualization
scaled_df = pd.DataFrame(X_scaled, columns=X.columns)

scaled_df

y = data['label'] # assiging the target column

"""# Creating a SVM model"""

#Splitting the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Training a linear SVM
svm = SVC(kernel='linear', probability=True)  # We need probabilities for AUC
svm.fit(X_train, y_train)

# Predicting on the test set
y_pred = svm.predict(X_test)
y_prob = svm.decision_function(X_test)  # Get decision function scores for AUC

# Generating a confusion matrix
cm = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:")
print(cm)

# Computing the AUC
fpr, tpr, thresholds = roc_curve(y_test, y_prob)
roc_auc = auc(fpr, tpr)
print("AUC:", roc_auc)

# Plot ROC curve
plt.figure()
lw = 2
plt.plot(fpr, tpr, color='darkorange', lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC)')
plt.legend(loc="lower right")
plt.show()

"""The AUC Socre of the model is 0.99 which means that the model indicates better discrimination between the positive and negative classes.

# Creating a SVM model with Tuning, feature weightage,cross validation
"""

X = data.drop(columns=['label']) # Assigning a feature column

y = data['label'] # assigning the target column

X

y

"""Creating a svm model"""

from sklearn.model_selection import StratifiedKFold
import seaborn as sns

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Standardize numerical features
scaler = StandardScaler()
X_train_num = scaler.fit_transform(X_train.drop(columns=['text']))
X_test_num = scaler.transform(X_test.drop(columns=['text']))

# Convert Tokenized Text to Unigram Bag-of-Words representation
vectorizer = CountVectorizer()
X_train_bow = vectorizer.fit_transform(X_train['text'].apply(' '.join))
X_test_bow = vectorizer.transform(X_test['text'].apply(' '.join))

# Convert the sparse matrix to a dense one
X_train_bow_dense = X_train_bow.toarray()
X_test_bow_dense = X_test_bow.toarray()

# Combine the standardized numerical features and BoW representation
X_train_combined = np.hstack((X_train_num, X_train_bow_dense))
X_test_combined = np.hstack((X_test_num, X_test_bow_dense))

# Set up cross-validation
kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
metrics = {'accuracy': [], 'precision': [], 'recall': [], 'f1': [], 'roc_auc': []}

for train_idx, val_idx in kfold.split(X_train_combined, y_train):
    # Split data into training and validation sets
    X_train_fold = X_train_combined[train_idx]
    y_train_fold = y_train.iloc[train_idx]
    X_val_fold = X_train_combined[val_idx]
    y_val_fold = y_train.iloc[val_idx]

    # Train a Support Vector Machine (SVM) model
    svm = SVC(kernel='linear', probability=True, class_weight='balanced')
    svm.fit(X_train_fold, y_train_fold)

    # Make predictions on the validation set
    y_pred = svm.predict(X_val_fold)
    y_prob = svm.decision_function(X_val_fold)  # Get decision function scores for AUC

    # Evaluate the model and store metrics
    metrics['accuracy'].append(accuracy_score(y_val_fold, y_pred))
    metrics['precision'].append(precision_score(y_val_fold, y_pred))
    metrics['recall'].append(recall_score(y_val_fold, y_pred))
    metrics['f1'].append(f1_score(y_val_fold, y_pred))
    metrics['roc_auc'].append(roc_auc_score(y_val_fold, y_prob))

# Average the metrics across folds
avg_metrics = {key: np.mean(value) for key, value in metrics.items()}

# Print the metrics
for metric, value in avg_metrics.items():
    print(f"{metric}: {value:.4f}")

# Visualize ROC Curve for the last fold
fpr, tpr, thresholds = roc_curve(y_val_fold, y_prob)
roc_auc = auc(fpr, tpr)

plt.figure()
lw = 2
plt.plot(fpr, tpr, color='darkorange', lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC)')
plt.legend(loc="lower right")
plt.show()

# Visualize Confusion Matrix for the last fold
conf_matrix = confusion_matrix(y_val_fold, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix')
plt.show()

"""The AUC Socre of the model is 0.98 which means that the model indicates better discrimination between the positive and
negative classes.
"""

